{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Reference: https://github.com/pasus/Reinforcement-Learning-Book-Revision <br/>Code Reference: https://github.com/zhihanyang2022/pytorch-sac <br/> Modified the code so as to apply it to Unity Enviornment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense, Lambda, concatenate\n",
    "from tensorflow.python.keras.optimizers import adam_v2\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "\n",
    "from replaybuffer import ReplayBuffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "\n",
    "    def __init__(self, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "        self.std_bound = [1e-2, 1.0]\n",
    "\n",
    "        self.h1 = Dense(128, activation='relu')\n",
    "        self.h2 = Dense(64, activation='relu')\n",
    "        self.h3 = Dense(32, activation='relu')\n",
    "        self.h4 = Dense(16, activation='relu')\n",
    "        self.mu = Dense(action_dim, activation='tanh')\n",
    "        self.std = Dense(action_dim, activation='softplus')\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.h1(state)\n",
    "        x = self.h2(x)\n",
    "        x = self.h3(x)\n",
    "        x = self.h4(x)\n",
    "        mu = self.mu(x)\n",
    "        std = self.std(x)\n",
    "\n",
    "        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
    "\n",
    "        return mu, std\n",
    "        \n",
    "    def sample_normal(self, mu, std):\n",
    "        normal_prob = tfp.distributions.Normal(mu, std)\n",
    "        action = normal_prob.sample()\n",
    "        # action = tf.tanh(action)\n",
    "       \n",
    "        \n",
    "        #limiting the action value\n",
    "        log_pdf = normal_prob.log_prob(action)\n",
    "        log_pdf = tf.reduce_sum(log_pdf, 1, keepdims=True)\n",
    "       \n",
    "        return action, log_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.x1 = Dense(64, activation='relu')\n",
    "        self.a1 = Dense(64, activation='relu')\n",
    "        self.h2 = Dense(64, activation='relu')\n",
    "        self.h3 = Dense(32, activation='relu')\n",
    "        self.h4 = Dense(16, activation='relu')\n",
    "        self.q = Dense(1, activation='linear')\n",
    "\n",
    "\n",
    "    def call(self, state_action):\n",
    "        state = state_action[0]\n",
    "        action = state_action[1]\n",
    "        x = self.x1(state)\n",
    "        a = self.a1(action)\n",
    "        h = concatenate([x, a], axis=-1)\n",
    "        x = self.h2(h)\n",
    "        x = self.h3(x)\n",
    "        x = self.h4(x)\n",
    "        q = self.q(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACagent(object):\n",
    "\n",
    "    def __init__(self, N_STATES, N_ACTIONS):\n",
    "\n",
    "        # Hyperparameter\n",
    "        self.GAMMA = 0.99\n",
    "        self.BATCH_SIZE = 1000\n",
    "        self.BUFFER_SIZE = 10000\n",
    "        # int(2e6)\n",
    "        self.ACTOR_LEARNING_RATE = 0.0001\n",
    "        self.CRITIC_LEARNING_RATE = 0.001\n",
    "        self.TAU = 0.001\n",
    "        self.ALPHA = 0.5\n",
    "\n",
    "        # Observation space and Action space\n",
    "        self.state_dim = N_STATES\n",
    "        self.action_dim = N_ACTIONS\n",
    "\n",
    "\n",
    "        # 액터 신경망 및 Q1, Q2 타깃 Q1, Q2 신경망 생성\n",
    "        self.actor = Actor(self.action_dim)\n",
    "        self.actor.build(input_shape=(None, self.state_dim))\n",
    "\n",
    "        self.critic_1 = Critic()\n",
    "        self.target_critic_1 = Critic()\n",
    "\n",
    "        self.critic_2 = Critic()\n",
    "        self.target_critic_2 = Critic()\n",
    "\n",
    "        state_in = Input((self.state_dim,))\n",
    "        action_in = Input((self.action_dim,))\n",
    "        self.critic_1([state_in, action_in])\n",
    "        self.target_critic_1([state_in, action_in])\n",
    "        self.critic_2([state_in, action_in])\n",
    "        self.target_critic_2([state_in, action_in])\n",
    "\n",
    "        self.actor.summary()\n",
    "        self.critic_1.summary()\n",
    "        self.critic_2.summary()\n",
    "\n",
    "        # 옵티마이저\n",
    "        Adam = adam_v2.Adam(learning_rate=self.ACTOR_LEARNING_RATE)\n",
    "        self.actor_opt = Adam\n",
    "        self.critic_1_opt = Adam\n",
    "        self.critic_2_opt = Adam\n",
    "\n",
    "        # 리플레이 버퍼 초기화\n",
    "        self.buffer = ReplayBuffers(self.BUFFER_SIZE)\n",
    "\n",
    "        # 에피소드에서 얻은 총 보상값을 저장하기 위한 변수\n",
    "        self.save_epi_reward = []\n",
    "\n",
    "        self.policy_lost = []\n",
    "        self.reward_list = []\n",
    "    \n",
    "    \n",
    "    ## 행동 샘플링\n",
    "    def get_action(self, state):\n",
    "        mu, std = self.actor(state)\n",
    "        action, _ = self.actor.sample_normal(mu, std)\n",
    "        # print(action.numpy())\n",
    "        # print(action.numpy()[0][0])\n",
    "        # print(action.numpy()[0][1])\n",
    "        # print()\n",
    "        return action.numpy()\n",
    "\n",
    "\n",
    "    ## 신경망의 파라미터값을 타깃 신경망으로 복사\n",
    "    def update_target_network(self, TAU):\n",
    "        phi_1 = self.critic_1.get_weights()\n",
    "        phi_2 = self.critic_2.get_weights()\n",
    "        target_phi_1 = self.target_critic_1.get_weights()\n",
    "        target_phi_2 = self.target_critic_2.get_weights()\n",
    "        for i in range(len(phi_1)):\n",
    "            target_phi_1[i] = TAU * phi_1[i] + (1 - TAU) * target_phi_1[i]\n",
    "            target_phi_2[i] = TAU * phi_2[i] + (1 - TAU) * target_phi_2[i]\n",
    "        self.target_critic_1.set_weights(target_phi_1)\n",
    "        self.target_critic_2.set_weights(target_phi_2)\n",
    "\n",
    "\n",
    "    ## Q1, Q2 신경망 학습\n",
    "    def critic_learn(self, states, actions, q_targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_1 = self.critic_1([states, actions], training=True)\n",
    "            loss_1 = tf.reduce_mean(tf.square(q_1-q_targets))\n",
    "\n",
    "        grads_1 = tape.gradient(loss_1, self.critic_1.trainable_variables)\n",
    "        self.critic_1_opt.apply_gradients(zip(grads_1, self.critic_1.trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_2 = self.critic_2([states, actions], training=True)\n",
    "            loss_2 = tf.reduce_mean(tf.square(q_2-q_targets))\n",
    "\n",
    "        grads_2 = tape.gradient(loss_2, self.critic_2.trainable_variables)\n",
    "        self.critic_2_opt.apply_gradients(zip(grads_2, self.critic_2.trainable_variables))\n",
    "\n",
    "\n",
    "    ## 액터 신경망 학습\n",
    "    def actor_learn(self, states):\n",
    "        with tf.GradientTape() as tape:\n",
    "            mu, std = self.actor(states, training=True)\n",
    "            actions, log_pdfs = self.actor.sample_normal(mu, std)\n",
    "            log_pdfs = tf.squeeze(log_pdfs, 1)\n",
    "            soft_q_1 = self.critic_1([states, actions])\n",
    "            soft_q_2 = self.critic_2([states, actions])\n",
    "            soft_q = tf.math.minimum(soft_q_1, soft_q_2)\n",
    "\n",
    "            loss = tf.reduce_mean(self.ALPHA * log_pdfs - soft_q)\n",
    "\n",
    "        grads = tape.gradient(loss, self.actor.trainable_variables)\n",
    "        self.actor_opt.apply_gradients(zip(grads, self.actor.trainable_variables))\n",
    "        return float(loss)\n",
    "\n",
    "\n",
    "    ## 시간차 타깃 계산\n",
    "    def q_target(self, rewards, q_values, dones):\n",
    "        y_k = np.asarray(q_values)\n",
    "        for i in range(q_values.shape[0]): # number of batch\n",
    "            if dones[i]:\n",
    "                y_k[i] = rewards[i]\n",
    "            else:\n",
    "                y_k[i] = rewards[i] + self.GAMMA * q_values[i]\n",
    "        return y_k\n",
    "\n",
    "\n",
    "    ## 신경망 파라미터 로드\n",
    "    def load_weights(self, path):\n",
    "        self.actor.load_weights(path + 'ParkingEnv_actor_2q.h5')\n",
    "        self.critic_1.load_weights(path + 'ParkingEnv_critic_12q.h5')\n",
    "        self.critic_2.load_weights(path + 'ParkingEnv_critic_22q.h5')\n",
    "\n",
    "\n",
    "    def train(self, max_episode_num, env, behavior_name):\n",
    "\n",
    "        cnt = 0\n",
    "        # 타깃 신경망 초기화\n",
    "        self.update_target_network(1.0)\n",
    "\n",
    "        # 에피소드마다 다음을 반복\n",
    "        for ep in range(int(max_episode_num)):\n",
    "            # 에피소드 초기화\n",
    "            # time, episode_reward, done = 0, 0, False\n",
    "            frame, episode_reward = 0, 0\n",
    "            # reset the enviornment\n",
    "            env.reset()\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            episode_done = False\n",
    "            # 환경 초기화 및 초기 상태 관측\n",
    "            # setting up the initial state as an array\n",
    "            x = decision_steps.obs[0][0] # Ray Perception 3D\n",
    "            y = decision_steps.obs[1][0] # Target location\n",
    "            state = np.concatenate((x, y), 0)\n",
    "            # state = decision_steps.obs[0][0]\n",
    "\n",
    "            while not episode_done:\n",
    "                # 행동 샘플링\n",
    "                action = self.get_action(tf.convert_to_tensor([state], dtype=tf.float32))\n",
    "                # 행동 범위 클리핑\n",
    "                # action = np.clip(action, -self.action_bound, self.action_bound)\n",
    "                # 다음 상태, 보상 관측\n",
    "                # next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                action = ActionTuple(np.array(action, dtype = np.float32))\n",
    "                env.set_actions(behavior_name, action)\n",
    "                env.step()\n",
    "                action = action._continuous # converting ActionTuple to array\n",
    "                next_decision_steps, next_terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "\n",
    "                # if the agent is still on, collect data and add it to buffer.\n",
    "                if next_decision_steps:\n",
    "                    train_reward = next_decision_steps.reward[0]\n",
    "                    x = next_decision_steps.obs[0][0]\n",
    "                    y = next_decision_steps.obs[1][0]\n",
    "                    next_state = np.concatenate((x, y), 0)\n",
    "                    # next_state = next_decision_steps.obs[0][0]\n",
    "                    episode_reward += next_decision_steps.reward[0]\n",
    "                    # episode_reward += -frame\n",
    "                    self.buffer.add_data(state, action, train_reward, next_state, False)\n",
    "                    episode_done = False\n",
    "\n",
    "                # if the agent is off, collect data and add True for done.\n",
    "                if next_terminal_steps:\n",
    "                    train_reward = next_terminal_steps.reward[0]\n",
    "                    x = next_terminal_steps.obs[0][0]\n",
    "                    y = next_terminal_steps.obs[1][0]\n",
    "                    next_state = np.concatenate((x, y), 0)\n",
    "                    # next_state = next_terminal_steps.obs[0][0]\n",
    "                    episode_reward += next_terminal_steps.reward[0]\n",
    "                    # episode_reward += -frame\n",
    "                    self.buffer.add_data(state, action, train_reward, next_state, True)\n",
    "                    episode_done = True\n",
    "\n",
    "\n",
    "                # 리플레이 버퍼가 일정 부분 채워지면 학습 진행 and ep % 3 == 0\n",
    "                if self.buffer.buffer_count() > self.BATCH_SIZE:\n",
    "\n",
    "                    # 리플레이 버퍼에서 샘플 무작위 추출\n",
    "                    states, actions, rewards, next_states, dones = self.buffer.sample_batch(self.BATCH_SIZE)\n",
    "\n",
    "                    # Q 타깃 계산\n",
    "                    next_mu, next_std = self.actor(tf.convert_to_tensor(next_states, dtype=tf.float32))\n",
    "                    next_actions, next_log_pdf = self.actor.sample_normal(next_mu, next_std)\n",
    "\n",
    "                    target_qs_1 = self.target_critic_1([next_states, next_actions])\n",
    "                    target_qs_2 = self.target_critic_2([next_states, next_actions])\n",
    "                    target_qs = tf.math.minimum(target_qs_1, target_qs_2)\n",
    "\n",
    "                    target_qi = target_qs - self.ALPHA * next_log_pdf\n",
    "\n",
    "                    # TD 타깃 계산\n",
    "                    y_i = self.q_target(rewards, target_qi.numpy(), dones)\n",
    "\n",
    "                    # Q1, Q2 신경망 업데이트\n",
    "                    self.critic_learn(tf.convert_to_tensor(states, dtype=tf.float32),\n",
    "                                      tf.convert_to_tensor(actions, dtype=tf.float32),\n",
    "                                      tf.convert_to_tensor(y_i, dtype=tf.float32))\n",
    "\n",
    "                    # 액터 신경망 업데이트\n",
    "                    policy_loss = self.actor_learn(tf.convert_to_tensor(states, dtype=tf.float32))\n",
    "                    \n",
    "                    if cnt % 500 == 0:\n",
    "                        self.reward_list.append(train_reward)\n",
    "                        self.policy_lost.append(policy_loss)\n",
    "                    # 타깃 신경망 업데이트\n",
    "                    self.update_target_network(self.TAU)\n",
    "                # 다음 스텝 준비\n",
    "                state = next_state\n",
    "                frame += 1\n",
    "                cnt += 1\n",
    "        \n",
    "\n",
    "            # 에피소드마다 결과 보상값 출력\n",
    "            print('Episode: ', ep+1, 'Frame: ', frame, 'u Reward: ', episode_reward/frame)\n",
    "        \n",
    "\n",
    "\n",
    "            # 에피소드마다 신경망 파라미터를 파일에 저장\n",
    "            self.actor.save_weights(\"./save_weights/ParkingEnv_actor_2q.h5\")\n",
    "            self.critic_1.save_weights(\"./save_weights/ParkingEnv_critic_12q.h5\")\n",
    "            self.critic_2.save_weights(\"./save_weights/ParkingEnv_critic_22q.h5\")\n",
    "\n",
    "        # 학습이 끝난 후, 누적 보상값 저장\n",
    "        np.savetxt('./save_weights/ParkingEnv_epi_reward_2q.txt', self.save_epi_reward)\n",
    "        print(self.save_epi_reward)\n",
    "\n",
    "    ## 에피소드와 누적 보상값을 그려주는 함수\n",
    "    def plot_result(self):\n",
    "        fig=plt.figure(figsize=(18, 6))\n",
    "        fig.add_subplot(1, 3, 1)  # 1 row, 3 columns\n",
    "        plt.plot(self.reward_list)\n",
    "\n",
    "        fig.add_subplot(1, 3, 3) \n",
    "        plt.plot(self.policy_lost)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run UE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ACTIONS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name= \"../Parking lot\", base_port=5004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CarAgent?team=0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "behavior_name = behavior_names[0]\n",
    "print(behavior_name)\n",
    "decision_steps, terminal_steps = env.get_steps(behavior_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the length of the space size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATES = len(decision_steps.obs[0][0]) + len(decision_steps.obs[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 배치 사이즈 늘리고\n",
    "## 저장 데이터 늘리고\n",
    "## 중간에 + 리워드 추가!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SACagent(N_STATES, N_ACTIONS)\n",
    "agent.train(30000, env, behavior_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name= \"../Parking lot\", base_port=5004)\n",
    "training = False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CarAgent?team=0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "behavior_name = behavior_names[0]\n",
    "print(behavior_name)\n",
    "decision_steps, terminal_steps = env.get_steps(behavior_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ACTIONS = 2\n",
    "N_STATES = len(decision_steps.obs[0][0]) + len(decision_steps.obs[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SACagent(N_STATES, N_ACTIONS)\n",
    "print(\"loading weights...\")\n",
    "agent.load_weights('./save_weights/')\n",
    "print(\"loaded weights!\")\n",
    "cnt = 0\n",
    "while True:\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    x = decision_steps.obs[0][0] # Ray Perception 3D\n",
    "    y = decision_steps.obs[1][0] # Target location\n",
    "    state = np.concatenate((x, y), 0)\n",
    "\n",
    "    action = agent.actor(tf.convert_to_tensor([state], dtype=tf.float32))[0]\n",
    "    print(action)\n",
    "    action = ActionTuple(np.array(action, dtype = np.float32))\n",
    "    env.set_actions(behavior_name, action)\n",
    "    env.step()\n",
    "    \n",
    "    cnt += 1\n",
    "\n",
    "    if cnt % 10000 ==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12c3f353b32d15211a66f0618d93d2675104f43f9733b916071b5e0f21aad9d2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
