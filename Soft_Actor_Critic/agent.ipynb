{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Reference: https://github.com/pasus/Reinforcement-Learning-Book-Revision <br/>Code Reference: https://github.com/zhihanyang2022/pytorch-sac <br/> Modified the code so as to apply it to Unity Enviornment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense, Lambda, concatenate\n",
    "from tensorflow.python.keras.optimizers import adam_v2\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "\n",
    "from replaybuffer import ReplayBuffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "    \n",
    "    def __init__(self, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "        self.std_bound = [1e-2, 1.0]\n",
    "\n",
    "        self.h1 = Dense(128, activation='relu')\n",
    "        self.h2 = Dense(64, activation='relu')\n",
    "        self.h3 = Dense(32, activation='relu')\n",
    "        self.h4 = Dense(16, activation='relu')\n",
    "        self.mu = Dense(action_dim, activation='tanh')\n",
    "        self.std = Dense(action_dim, activation='softplus')\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.h1(state)\n",
    "        x = self.h2(x)\n",
    "        x = self.h3(x)\n",
    "        x = self.h4(x)\n",
    "        mu = self.mu(x)\n",
    "        std = self.std(x)\n",
    "\n",
    "        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
    "\n",
    "        return mu, std\n",
    "        \n",
    "    def sample_normal(self, mu, std):\n",
    "        normal_prob = tfp.distributions.Normal(mu, std)\n",
    "        action = normal_prob.sample()\n",
    "        # here, action could be squeezed, but in this project, agent's velocity is included in the observation space.\n",
    "        # it is therefore, your choice whether or not to include tf.tanh. \n",
    "        # action = tf.tanh(action)\n",
    "       \n",
    "        \n",
    "        #limiting the action value\n",
    "        log_pdf = normal_prob.log_prob(action)\n",
    "        log_pdf = tf.reduce_sum(log_pdf, 1, keepdims=True)\n",
    "       \n",
    "        return action, log_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.x1 = Dense(64, activation='relu')\n",
    "        self.a1 = Dense(64, activation='relu')\n",
    "        self.h2 = Dense(64, activation='relu')\n",
    "        self.h3 = Dense(32, activation='relu')\n",
    "        self.h4 = Dense(16, activation='relu')\n",
    "        self.q = Dense(1, activation='linear')\n",
    "\n",
    "\n",
    "    def call(self, state_action):\n",
    "        state = state_action[0]\n",
    "        action = state_action[1]\n",
    "        x = self.x1(state)\n",
    "        a = self.a1(action)\n",
    "        h = concatenate([x, a], axis=-1)\n",
    "        x = self.h2(h)\n",
    "        x = self.h3(x)\n",
    "        x = self.h4(x)\n",
    "        q = self.q(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACagent(object):\n",
    "\n",
    "    def __init__(self, N_STATES, N_ACTIONS):\n",
    "\n",
    "        # Hyperparameter\n",
    "        self.GAMMA = 0.99\n",
    "        self.BATCH_SIZE = 1000\n",
    "        self.BUFFER_SIZE = 10000\n",
    "        self.ACTOR_LEARNING_RATE = 0.0001\n",
    "        self.CRITIC_LEARNING_RATE = 0.001\n",
    "        self.TAU = 0.001\n",
    "        self.ALPHA = 0.5\n",
    "\n",
    "        # Observation space and Action space\n",
    "        self.state_dim = N_STATES\n",
    "        self.action_dim = N_ACTIONS\n",
    "\n",
    "\n",
    "        # Build Actor, Q1, Q2 and its Target NN\n",
    "        self.actor = Actor(self.action_dim)\n",
    "        self.actor.build(input_shape=(None, self.state_dim))\n",
    "\n",
    "        self.critic_1 = Critic()\n",
    "        self.target_critic_1 = Critic()\n",
    "\n",
    "        self.critic_2 = Critic()\n",
    "        self.target_critic_2 = Critic()\n",
    "\n",
    "        state_in = Input((self.state_dim,))\n",
    "        action_in = Input((self.action_dim,))\n",
    "        self.critic_1([state_in, action_in])\n",
    "        self.target_critic_1([state_in, action_in])\n",
    "        self.critic_2([state_in, action_in])\n",
    "        self.target_critic_2([state_in, action_in])\n",
    "\n",
    "        self.actor.summary()\n",
    "        self.critic_1.summary()\n",
    "        self.critic_2.summary()\n",
    "\n",
    "        # optimizer, any other form of optimizer should also work. \n",
    "        Adam = adam_v2.Adam(learning_rate=self.ACTOR_LEARNING_RATE)\n",
    "        self.actor_opt = Adam\n",
    "        self.critic_1_opt = Adam\n",
    "        self.critic_2_opt = Adam\n",
    "\n",
    "        # clear out the buffer\n",
    "        self.buffer = ReplayBuffers(self.BUFFER_SIZE)\n",
    "\n",
    "        # for plotting purposes, data is stored. \n",
    "        self.policy_lost = []\n",
    "        self.reward_list = []\n",
    "    \n",
    "    \n",
    "    ## get a sample action \n",
    "    def get_action(self, state):\n",
    "        mu, std = self.actor(state)\n",
    "        action, _ = self.actor.sample_normal(mu, std)\n",
    "        return action.numpy()\n",
    "\n",
    "    ## copy NN parameter values to the target NN.\n",
    "    def update_target_network(self, TAU):\n",
    "        phi_1 = self.critic_1.get_weights()\n",
    "        phi_2 = self.critic_2.get_weights()\n",
    "        target_phi_1 = self.target_critic_1.get_weights()\n",
    "        target_phi_2 = self.target_critic_2.get_weights()\n",
    "        for i in range(len(phi_1)):\n",
    "            target_phi_1[i] = TAU * phi_1[i] + (1 - TAU) * target_phi_1[i]\n",
    "            target_phi_2[i] = TAU * phi_2[i] + (1 - TAU) * target_phi_2[i]\n",
    "        self.target_critic_1.set_weights(target_phi_1)\n",
    "        self.target_critic_2.set_weights(target_phi_2)\n",
    "\n",
    "\n",
    "    ## train Q1, Q2\n",
    "    def critic_learn(self, states, actions, q_targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_1 = self.critic_1([states, actions], training=True)\n",
    "            loss_1 = tf.reduce_mean(tf.square(q_1-q_targets))\n",
    "\n",
    "        grads_1 = tape.gradient(loss_1, self.critic_1.trainable_variables)\n",
    "        self.critic_1_opt.apply_gradients(zip(grads_1, self.critic_1.trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_2 = self.critic_2([states, actions], training=True)\n",
    "            loss_2 = tf.reduce_mean(tf.square(q_2-q_targets))\n",
    "\n",
    "        grads_2 = tape.gradient(loss_2, self.critic_2.trainable_variables)\n",
    "        self.critic_2_opt.apply_gradients(zip(grads_2, self.critic_2.trainable_variables))\n",
    "\n",
    "\n",
    "    ## Train the actor NN.\n",
    "    def actor_learn(self, states):\n",
    "        with tf.GradientTape() as tape:\n",
    "            mu, std = self.actor(states, training=True)\n",
    "            actions, log_pdfs = self.actor.sample_normal(mu, std)\n",
    "            log_pdfs = tf.squeeze(log_pdfs, 1)\n",
    "            soft_q_1 = self.critic_1([states, actions])\n",
    "            soft_q_2 = self.critic_2([states, actions])\n",
    "            soft_q = tf.math.minimum(soft_q_1, soft_q_2)\n",
    "\n",
    "            loss = tf.reduce_mean(self.ALPHA * log_pdfs - soft_q)\n",
    "\n",
    "        grads = tape.gradient(loss, self.actor.trainable_variables)\n",
    "        self.actor_opt.apply_gradients(zip(grads, self.actor.trainable_variables))\n",
    "        return float(loss)\n",
    "\n",
    "\n",
    "    ## calculating the target\n",
    "    def q_target(self, rewards, q_values, dones):\n",
    "        y_k = np.asarray(q_values)\n",
    "        for i in range(q_values.shape[0]): # number of batch\n",
    "            if dones[i]:\n",
    "                y_k[i] = rewards[i]\n",
    "            else:\n",
    "                y_k[i] = rewards[i] + self.GAMMA * q_values[i]\n",
    "        return y_k\n",
    "\n",
    "\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        self.actor.load_weights(path + 'ParkingEnv_actor_2q.h5')\n",
    "        self.critic_1.load_weights(path + 'ParkingEnv_critic_12q.h5')\n",
    "        self.critic_2.load_weights(path + 'ParkingEnv_critic_22q.h5')\n",
    "\n",
    "\n",
    "    def train(self, max_episode_num, env, behavior_name):\n",
    "\n",
    "        cnt = 0\n",
    "        # reset target network param.\n",
    "        self.update_target_network(1.0)\n",
    "\n",
    "\n",
    "        for ep in range(int(max_episode_num)):\n",
    "            frame, episode_reward = 0, 0\n",
    "            # reset the enviornment\n",
    "            env.reset()\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            episode_done = False\n",
    "            # setting up the initial state as an array\n",
    "            x = decision_steps.obs[0][0] # Ray Perception 3D\n",
    "            y = decision_steps.obs[1][0] # Agent's velocity x,z\n",
    "            state = np.concatenate((x, y), 0)\n",
    "\n",
    "            while not episode_done:\n",
    "        \n",
    "                action = self.get_action(tf.convert_to_tensor([state], dtype=tf.float32))\n",
    "                # wrap the action with ActionTuple before sending it to UE. \n",
    "                action = ActionTuple(np.array(action, dtype = np.float32))\n",
    "                env.set_actions(behavior_name, action)\n",
    "                # move the agent along with the action. \n",
    "                env.step()\n",
    "                action = action._continuous # converting ActionTuple to array\n",
    "                next_decision_steps, next_terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "\n",
    "                # if the agent is still on, collect data and add it to buffer.\n",
    "                if next_decision_steps:\n",
    "                    # get the reward. \n",
    "                    train_reward = next_decision_steps.reward[0]\n",
    "                    x = next_decision_steps.obs[0][0]\n",
    "                    y = next_decision_steps.obs[1][0]\n",
    "                    next_state = np.concatenate((x, y), 0)\n",
    "                    episode_reward += next_decision_steps.reward[0]\n",
    "                    # store the data to the buffer\n",
    "                    self.buffer.add_data(state, action, train_reward, next_state, False)\n",
    "                    episode_done = False\n",
    "\n",
    "                # if the agent is off, collect data and add True for done.\n",
    "                if next_terminal_steps:\n",
    "                    # get the reward. \n",
    "                    train_reward = next_terminal_steps.reward[0]\n",
    "                    x = next_terminal_steps.obs[0][0]\n",
    "                    y = next_terminal_steps.obs[1][0]\n",
    "                    next_state = np.concatenate((x, y), 0)\n",
    "                    episode_reward += next_terminal_steps.reward[0]\n",
    "                    # store the data to the buffer\n",
    "                    self.buffer.add_data(state, action, train_reward, next_state, True)\n",
    "                    episode_done = True\n",
    "\n",
    "\n",
    "                # if buffer has enough data start training. \n",
    "                if self.buffer.buffer_count() > self.BATCH_SIZE:\n",
    "\n",
    "                    \n",
    "                    states, actions, rewards, next_states, dones = self.buffer.sample_batch(self.BATCH_SIZE)\n",
    "\n",
    "                    # Calculate the Q target value\n",
    "                    next_mu, next_std = self.actor(tf.convert_to_tensor(next_states, dtype=tf.float32))\n",
    "                    next_actions, next_log_pdf = self.actor.sample_normal(next_mu, next_std)\n",
    "\n",
    "                    target_qs_1 = self.target_critic_1([next_states, next_actions])\n",
    "                    target_qs_2 = self.target_critic_2([next_states, next_actions])\n",
    "                    target_qs = tf.math.minimum(target_qs_1, target_qs_2)\n",
    "\n",
    "                    target_qi = target_qs - self.ALPHA * next_log_pdf\n",
    "                    y_i = self.q_target(rewards, target_qi.numpy(), dones)\n",
    "                    self.critic_learn(tf.convert_to_tensor(states, dtype=tf.float32),\n",
    "                                      tf.convert_to_tensor(actions, dtype=tf.float32),\n",
    "                                      tf.convert_to_tensor(y_i, dtype=tf.float32))\n",
    "\n",
    "                    # update Actor and return policy loss\n",
    "                    policy_loss = self.actor_learn(tf.convert_to_tensor(states, dtype=tf.float32))\n",
    "                    \n",
    "                    # store the performance of the algorithm.\n",
    "                    if cnt % 500 == 0:\n",
    "                        self.reward_list.append(train_reward)\n",
    "                        self.policy_lost.append(policy_loss)\n",
    "                    self.update_target_network(self.TAU)\n",
    "                state = next_state\n",
    "                frame += 1\n",
    "                cnt += 1\n",
    "        \n",
    "\n",
    "            # Episode output\n",
    "            print('Episode: ', ep+1, 'Frame: ', frame, 'u Reward: ', episode_reward/frame)\n",
    "        \n",
    "\n",
    "\n",
    "            # save weights for each run\n",
    "            self.actor.save_weights(\"./save_weights/ParkingEnv_actor_2q.h5\")\n",
    "            self.critic_1.save_weights(\"./save_weights/ParkingEnv_critic_12q.h5\")\n",
    "            self.critic_2.save_weights(\"./save_weights/ParkingEnv_critic_22q.h5\")\n",
    "\n",
    "    \n",
    "    def plot_result(self):\n",
    "        fig=plt.figure(figsize=(18, 6))\n",
    "        fig.add_subplot(1, 3, 1)  # 1 row, 3 columns\n",
    "        plt.plot(self.reward_list)\n",
    "\n",
    "        fig.add_subplot(1, 3, 3) \n",
    "        plt.plot(self.policy_lost)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Unity Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ACTIONS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name= \"../Parking lot\", base_port=5004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "behavior_name = behavior_names[0]\n",
    "decision_steps, terminal_steps = env.get_steps(behavior_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the length of the space size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial states\n",
    "N_STATES = len(decision_steps.obs[0][0]) + len(decision_steps.obs[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SACagent(N_STATES, N_ACTIONS)\n",
    "# usually 30K is enough. \n",
    "agent.train(30000, env, behavior_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "** *When the training is finished, the env will automatically re-open and load the saved weights. <br/> Note, due to oscillation during the experiment, poor parameters could be loaded. Be careful with your training. Normally, it is better to stop when the agent starts to behave well in the enviornment.* **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name= \"../Parking lot\", base_port=5004)\n",
    "training = False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "behavior_names = list(env.behavior_specs.keys())\n",
    "behavior_name = behavior_names[0]\n",
    "print(behavior_name)\n",
    "decision_steps, terminal_steps = env.get_steps(behavior_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ACTIONS = 2\n",
    "N_STATES = len(decision_steps.obs[0][0]) + len(decision_steps.obs[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SACagent(N_STATES, N_ACTIONS)\n",
    "print(\"loading weights...\")\n",
    "agent.load_weights('./save_weights/')\n",
    "print(\"loaded weights!\")\n",
    "cnt = 0\n",
    "while True:\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    x = decision_steps.obs[0][0] # Ray Perception 3D\n",
    "    y = decision_steps.obs[1][0] # Agent's velocity x,z\n",
    "    state = np.concatenate((x, y), 0)\n",
    "\n",
    "    action = agent.actor(tf.convert_to_tensor([state], dtype=tf.float32))[0]\n",
    "    print(action)\n",
    "    action = ActionTuple(np.array(action, dtype = np.float32))\n",
    "    env.set_actions(behavior_name, action)\n",
    "    env.step()\n",
    "    \n",
    "    cnt += 1\n",
    "\n",
    "    if cnt % 10000 ==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12c3f353b32d15211a66f0618d93d2675104f43f9733b916071b5e0f21aad9d2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
